@ -1,161 +0,0 @@
#!/usr/bin/env bash
### BEGIN MODULE INFO
# Name:                  sentinel_chat
# Short-Description:     Conversational AI assistant for shell queries
# Description:           Provides a conversational AI assistant to help with shell
#                        commands, questions, and system information retrieval.
# Author:                John
# URL:                   https://github.com/SWORDIntel/SENTINEL/
# Version:               1.0.0
# Stability:             stable
# Tags:                  chat, ai, assistant, llm
# Provides:              ai_chat_assistant
# Requires:              python3, llama-cpp-python, rich
# Conflicts:             none
### END MODULE INFO

# Check if module is enabled in bashrc.postcustom
if [[ "${SENTINEL_CHAT_ENABLED:-0}" != "1" ]]; then
    echo "SENTINEL Chat module is disabled."
    echo "To enable, add the following line to your ~/.bashrc.postcustom file:"
    echo "  export SENTINEL_CHAT_ENABLED=1"
    return 0
fi

# Debug function to conditionally print messages
sentinel_debug() {
    if [ "${SENTINEL_DEBUG:-0}" -eq 1 ]; then
        echo "[SENTINEL DEBUG] $*"
    fi
}

# Check dependencies
if command -v python3 &>/dev/null; then
    # Check for required Python packages
    if python3 -c "import llama_cpp" &>/dev/null && \
       python3 -c "import rich" &>/dev/null; then
        # Module is already enabled via bashrc.postcustom
        true
    else
        echo "Info: llama-cpp-python or rich not installed."
        echo "To enable SENTINEL Chat, run: sentinel_chat_install_deps"
        return 0
    fi
else
    echo "Warning: python3 not found. SENTINEL Chat capabilities disabled."
    return 0
fi

# Ensure the .sentinel directory exists
if [ ! -d "${HOME}/.sentinel" ]; then
    mkdir -p "${HOME}/.sentinel"
    mkdir -p "${HOME}/.sentinel/models"
    sentinel_debug "Created ${HOME}/.sentinel directory structure"
fi

# Path to Chat script - try installed version first, then fall back to development location
INSTALLED_CHAT_SCRIPT="${HOME}/.sentinel/sentinel_chat.py"
CONTRIB_CHAT_SCRIPT="$(dirname "$(dirname "${BASH_SOURCE[0]}")")/contrib/sentinel_chat.py"

# Determine which script to use
if [ -f "$INSTALLED_CHAT_SCRIPT" ]; then
    SENTINEL_CHAT_SCRIPT="$INSTALLED_CHAT_SCRIPT"
    sentinel_debug "Using installed chat script at $SENTINEL_CHAT_SCRIPT"
elif [ -f "$CONTRIB_CHAT_SCRIPT" ]; then
    SENTINEL_CHAT_SCRIPT="$CONTRIB_CHAT_SCRIPT"
    sentinel_debug "Using development chat script at $SENTINEL_CHAT_SCRIPT"
else
    SENTINEL_CHAT_SCRIPT="$INSTALLED_CHAT_SCRIPT"  # Default to installed location
    sentinel_debug "Warning: Chat script not found. Module will attempt to use $SENTINEL_CHAT_SCRIPT"
fi

# Install dependencies for SENTINEL Chat
function sentinel_chat_install_deps() {
    if command -v python3 &>/dev/null; then
        echo "Installing required Python packages..."
        
        # Check if script exists, if not inform user
        if [ ! -f "$SENTINEL_CHAT_SCRIPT" ]; then
            echo "Warning: Chat script not found at $SENTINEL_CHAT_SCRIPT"
            echo "Installing dependencies manually..."
            pip install llama-cpp-python rich readline
        else
            python3 "$SENTINEL_CHAT_SCRIPT" --install-deps
        fi
        
        # Check if installation was successful
        if python3 -c "import llama_cpp" &>/dev/null && \
           python3 -c "import rich" &>/dev/null; then
            echo "Dependencies installed successfully."
            echo "SENTINEL Chat is now enabled."
        else
            echo "Error: Failed to install dependencies."
            echo "Please install manually: pip install llama-cpp-python rich readline"
        fi
    else
        echo "Error: python3 not found. Cannot install dependencies."
    fi
}

# Main chat function
function sentinel_chat() {
    if [ $SENTINEL_CHAT_ENABLED -eq 1 ]; then
        # Check if the script exists
        if [ ! -f "$SENTINEL_CHAT_SCRIPT" ]; then
            echo "Error: Chat script not found at $SENTINEL_CHAT_SCRIPT"
            echo "Please run the SENTINEL installer to set up the chat module."
            return 1
        fi
        
        # If arguments are provided, use them as a direct query
        if [ $# -gt 0 ]; then
            python3 "$SENTINEL_CHAT_SCRIPT" "$@"
        else
            # Interactive mode
            python3 "$SENTINEL_CHAT_SCRIPT"
        fi
    else
        echo "SENTINEL Chat is not enabled."
        echo "To enable, run: sentinel_chat_install_deps"
    fi
}

# Function to check model status
function sentinel_chat_status() {
    if [ $SENTINEL_CHAT_ENABLED -eq 1 ]; then
        MODEL_DIR=~/.sentinel/models
        CONFIG_FILE=~/.sentinel/chat_config.json
        
        echo "SENTINEL Chat Status:"
        echo "----------------------"
        echo "Enabled: Yes"
        
        if [ -f "$CONFIG_FILE" ]; then
            MODEL_NAME=$(grep "model\":" "$CONFIG_FILE" | cut -d'"' -f4)
            echo "Model: $MODEL_NAME"
            
            if [ -d "$MODEL_DIR" ]; then
                MODEL_PATH="$MODEL_DIR/$MODEL_NAME"
                if [ -f "$MODEL_PATH" ]; then
                    MODEL_SIZE=$(du -h "$MODEL_PATH" | cut -f1)
                    echo "Model size: $MODEL_SIZE"
                    echo "Model path: $MODEL_PATH"
                    echo "Status: Ready"
                else
                    echo "Status: Model not downloaded yet"
                fi
            fi
        else
            echo "Status: Configuration not initialized"
        fi
    else
        echo "SENTINEL Chat Status:"
        echo "----------------------"
        echo "Enabled: No"
        echo "Status: Dependencies not installed"
    fi
}

# Command aliases
alias schat="sentinel_chat"
alias sentinel-chat="sentinel_chat"     